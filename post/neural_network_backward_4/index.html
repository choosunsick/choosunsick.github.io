<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Chapter4 오차역전파법을 적용하기 - Sunsick&#39;s blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Sunsick" /><meta name="description" content="이 시리즈는 R로 딥러닝을 구현하고 설명하는 것에 목표를 둔 글입니다. 신경망 챕터에 이어서 3번째 순서입니다. 이전 글과 내용이 이어집니다. 이전 글들은 이곳에서" /><meta name="keywords" content="blog, R, Data Analysis" />






<meta name="generator" content="Hugo 0.53 with even 4.0.0" />


<link rel="canonical" href="https://choosunsick.github.io/post/neural_network_backward_4/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Chapter4 오차역전파법을 적용하기" />
<meta property="og:description" content="이 시리즈는 R로 딥러닝을 구현하고 설명하는 것에 목표를 둔 글입니다. 신경망 챕터에 이어서 3번째 순서입니다. 이전 글과 내용이 이어집니다. 이전 글들은 이곳에서" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://choosunsick.github.io/post/neural_network_backward_4/" /><meta property="article:published_time" content="2020-03-30T22:07:37&#43;09:00"/>
<meta property="article:modified_time" content="2020-03-30T22:07:37&#43;09:00"/>

<meta itemprop="name" content="Chapter4 오차역전파법을 적용하기">
<meta itemprop="description" content="이 시리즈는 R로 딥러닝을 구현하고 설명하는 것에 목표를 둔 글입니다. 신경망 챕터에 이어서 3번째 순서입니다. 이전 글과 내용이 이어집니다. 이전 글들은 이곳에서">


<meta itemprop="datePublished" content="2020-03-30T22:07:37&#43;09:00" />
<meta itemprop="dateModified" content="2020-03-30T22:07:37&#43;09:00" />
<meta itemprop="wordCount" content="2857">



<meta itemprop="keywords" content="R로 딥러닝하기,신경망,역전파," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Chapter4 오차역전파법을 적용하기"/>
<meta name="twitter:description" content="이 시리즈는 R로 딥러닝을 구현하고 설명하는 것에 목표를 둔 글입니다. 신경망 챕터에 이어서 3번째 순서입니다. 이전 글과 내용이 이어집니다. 이전 글들은 이곳에서"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Choo&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Choo&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Chapter4 오차역전파법을 적용하기</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-03-30 </span>
        <div class="post-category">
            <a href="/categories/r/"> R </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#오차역전파법을-적용한-2층-신경망-구현하기">오차역전파법을 적용한 2층 신경망 구현하기</a>
<ul>
<li><a href="#오차역전파법으로-구한-기울기-검증하기">오차역전파법으로 구한 기울기 검증하기</a></li>
<li><a href="#오차역전파법-사용한-학습-구현하기">오차역전파법 사용한 학습 구현하기</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>이 시리즈는 R로 딥러닝을 구현하고 설명하는 것에 목표를 둔 글입니다. 신경망 챕터에 이어서 3번째 순서입니다. <a href="https://choosunsick.github.io/post/neural_network_backward_3/">이전 글</a>과 내용이 이어집니다. 이전 글들은 <a href="https://choosunsick.github.io/post/contents_list/">이곳</a>에서 찾아 보실 수 있습니다.</p>

<h2 id="오차역전파법을-적용한-2층-신경망-구현하기">오차역전파법을 적용한 2층 신경망 구현하기</h2>

<p>이제 오차역전파법이 적용된 2층 신경망을 구현해보겠습니다. 역전파가 적용된 신경망 모델 역시 기존 순전파에서 진행한 학습 과정은 같습니다. 그렇지만 다시한번 전체 학습 과정을 살펴보면 다음과 같습니다. 학습 목표는 순전파와 동일하게 손실 함수의 값을 줄이고 최적의 가중치와 편향 값을 찾는 것입니다.</p>

<p>먼저 모델에 적용할 훈련데이터를 준비합니다. 훈련데이터 일부를 무작위로 가져와 학습에 사용하는 방법을 미니배치라고 하며, 이 방법을 사용할 것입니다. 다음으로 모델에서 무작위로 만들어진 가중치와 편향에 대한 기울기를 구합니다. 단 역전파에서는 수치미분이 아닌 오차역전파 방식으로 기울기를 구하게 됩니다. 다음으로는 가중치와 편향을 손실 함수 값이 작아지는 방향으로 갱신합니다. 이제 손실함수 값이 최소값이 되도록 이 과정을 반복해줍니다.</p>

<p>오차역전파 법을 적용한 2층 신경망을 직접 구현해봅시다. 첫번째 구현은 가중치와 편향을 랜덤으로 만들어주는 함수입니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r">TwoLayerNet <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>input_size<span class="p">,</span> hidden_size<span class="p">,</span> output_size<span class="p">,</span> weight_init_std <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span> <span class="p">{</span>
    W1 <span class="o">&lt;-</span> weight_init_std<span class="o">*</span><span class="kt">matrix</span><span class="p">(</span>rnorm<span class="p">(</span>n <span class="o">=</span> input_size<span class="o">*</span>hidden_size<span class="p">),</span> nrow <span class="o">=</span> input_size<span class="p">,</span> ncol <span class="o">=</span> hidden_size<span class="p">)</span>
    b1 <span class="o">&lt;-</span> <span class="kt">matrix</span><span class="p">(</span><span class="kp">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span>hidden_size<span class="p">),</span>nrow <span class="o">=</span> <span class="m">1</span><span class="p">,</span>ncol <span class="o">=</span> hidden_size<span class="p">)</span>
    W2 <span class="o">&lt;-</span> weight_init_std<span class="o">*</span><span class="kt">matrix</span><span class="p">(</span>rnorm<span class="p">(</span>n <span class="o">=</span> hidden_size<span class="o">*</span>output_size<span class="p">),</span> nrow <span class="o">=</span> hidden_size<span class="p">,</span> ncol <span class="o">=</span> output_size<span class="p">)</span>
    b2 <span class="o">&lt;-</span> <span class="kt">matrix</span><span class="p">(</span><span class="kp">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span>output_size<span class="p">),</span>nrow <span class="o">=</span> <span class="m">1</span><span class="p">,</span>ncol <span class="o">=</span> output_size<span class="p">)</span>
    params <span class="o">&lt;&lt;-</span> <span class="kt">list</span><span class="p">(</span>W1 <span class="o">=</span> W1<span class="p">,</span> b1 <span class="o">=</span> b1<span class="p">,</span> W2 <span class="o">=</span> W2<span class="p">,</span> b2 <span class="o">=</span> b2<span class="p">)</span>
    <span class="kr">return</span><span class="p">(</span><span class="kt">list</span><span class="p">(</span>input_size<span class="p">,</span> hidden_size<span class="p">,</span> output_size<span class="p">,</span>weight_init_std<span class="p">))</span>
<span class="p">}</span>

<span class="o">&gt;</span> TwoLayerNet<span class="p">(</span>input_size <span class="o">=</span> <span class="m">784</span><span class="p">,</span> hidden_size <span class="o">=</span> <span class="m">50</span><span class="p">,</span> output_size <span class="o">=</span> <span class="m">10</span><span class="p">)</span>

<span class="p">[[</span><span class="m">1</span><span class="p">]]</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">784</span>

<span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">50</span>

<span class="p">[[</span><span class="m">3</span><span class="p">]]</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">10</span>

<span class="p">[[</span><span class="m">4</span><span class="p">]]</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.01</span></code></pre></td></tr></table>
</div>
</div>
<p>이 함수는 구현하는 신경망의 층수에 따라 구현이 달라지는데 이 코드는 2층 신경망을 기준으로 구현했습니다. 2층 신경망은 2개의 가중치와 편향이 필요합니다. 기존 순전파의 초기값 설정과 다를바가 없습니다. 차이점은 각 층의 가중치와 편향 행렬을 리스트로 묶어 저장한다는 점 외에는 없습니다.</p>

<p>다음으로는 순전파 계산 과정에 필요한 함수들의 구현을 알아보겠습니다. 역전파는 먼저 순전파 계산이 진행된 후에 계산이 진행됩니다.</p>

<p>순전파와 역전파 계산에 필요한 함수는 활성화 함수인 <code>ReLU()</code>와 순전파에서 가중치 행렬와 편향 행렬의 계산을 하는 <code>affine()</code> 계층,  <code>softmax</code>와 <code>loss</code> 함수를 같이 묶은 <code>softmaxwithloss()</code>입니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="kn">source</span><span class="p">(</span><span class="s">&#34;./DeepLearningFromForR/functions.R&#34;</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<p>역전파 계산에 앞서 순전파 계산에 필요한 것으로는 손실 함수 계산에 필요한 model.backward 함수가 있습니다. 이 함수에 대한 구현은 다음과 같습니다. 이 함수는 함수의 이름만 변경 되었을 뿐 사실상 계산 구조는 순전파에서와 같습니다.</p>

<p>입력 신호와 첫 번째 가중치 및 편향을 계산하고, 이후 활성화 함수인 Relu 함수를 사용하여 다음 층의 입력 신호로 전달합니다. 다음층에서 마찬가지로 입력 신호와 두 번째 가중치와 편향 행렬을 계산합니다. 본래 순전파에서는 이 값에 softmax 함수도 사용했었지만 여기서는 softmax 함수의 적용은 softmaxwithloss 함수로 빠져있습니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r">model.backward <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">){</span>
    Affine_1_layer <span class="o">&lt;-</span> Affine.forward<span class="p">(</span>params<span class="o">$</span>W1<span class="p">,</span> params<span class="o">$</span>b1<span class="p">,</span> x<span class="p">)</span>
    Relu_1_layer <span class="o">&lt;-</span> Relu.forward<span class="p">(</span>Affine_1_layer<span class="o">$</span>out<span class="p">)</span>
    Affine_2_layer <span class="o">&lt;-</span> Affine.forward<span class="p">(</span>params<span class="o">$</span>W2<span class="p">,</span> params<span class="o">$</span>b2<span class="p">,</span> Relu_1_layer<span class="o">$</span>out<span class="p">)</span>
    <span class="kr">return</span><span class="p">(</span><span class="kt">list</span><span class="p">(</span>x <span class="o">=</span> Affine_2_layer<span class="o">$</span>out<span class="p">,</span> Affine_1.forward <span class="o">=</span> Affine_1_layer<span class="p">,</span> Affine_2.forward <span class="o">=</span> Affine_2_layer<span class="p">,</span> Relu_1.forward <span class="o">=</span> Relu_1_layer<span class="p">))</span>
<span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>손실 함수 계산 역시 순전파 때랑 크게 달라진게 없습니다. 단지 backward_loss 함수에서 softmax와 cross_entropy_error 함수를 적용하는 코드가 생겼을 뿐입니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r">backward_loss <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> <span class="kp">t</span><span class="p">){</span>
    temp <span class="o">&lt;-</span> model.backward<span class="p">(</span>x<span class="p">)</span>
    y <span class="o">&lt;-</span> temp<span class="o">$</span>x
    last_layer.forward <span class="o">&lt;-</span> SoftmaxWithLoss.forward<span class="p">(</span>y<span class="p">,</span> <span class="kp">t</span><span class="p">)</span>
    <span class="kr">return</span><span class="p">(</span><span class="kt">list</span><span class="p">(</span>loss <span class="o">=</span> last_layer.forward<span class="o">$</span>loss<span class="p">,</span> softmax <span class="o">=</span> last_layer.forward<span class="p">,</span> predict <span class="o">=</span>  temp<span class="p">))</span>
<span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>다음으로는 순전파 계산과 가장 큰 차이가 있는 gradient 함수입니다. 기존의 순전파 계산은 backward_loss 함수를 사용하는 반면 역전파 계산은 마지막층에서부터 첫 번째 층으로 역순으로 올라갑니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r">gradient <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> <span class="kp">t</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1"># 순전파</span>
    temp_loss <span class="o">&lt;-</span> backward_loss<span class="p">(</span>x<span class="p">,</span> <span class="kp">t</span><span class="p">)</span>
    <span class="c1">#역전파</span>
    dout <span class="o">&lt;-</span> <span class="m">1</span>
    last_layer.backward <span class="o">&lt;-</span> SoftmaxWithLoss.backward<span class="p">(</span>temp_loss<span class="o">$</span>softmax<span class="p">,</span> dout<span class="p">)</span>
    Affine_2_layer.backward <span class="o">&lt;-</span> Affine.backward<span class="p">(</span>temp_loss<span class="o">$</span>predict<span class="o">$</span>Affine_2.forward<span class="p">,</span> dout <span class="o">=</span> last_layer.backward<span class="o">$</span>dx<span class="p">)</span>
    Relu_1_layer.backward <span class="o">&lt;-</span> Relu.backward<span class="p">(</span>temp_loss<span class="o">$</span>predict<span class="o">$</span>Relu_1.forward<span class="p">,</span> dout <span class="o">=</span> Affine_2_layer.backward<span class="o">$</span>dx<span class="p">)</span>
    Affine_1_layer.backward <span class="o">&lt;-</span> Affine.backward<span class="p">(</span>temp_loss<span class="o">$</span>predict<span class="o">$</span>Affine_1.forward<span class="p">,</span> dout <span class="o">=</span> Relu_1_layer.backward<span class="o">$</span>dx<span class="p">)</span>

    grads  <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">(</span>W1 <span class="o">=</span> Affine_1_layer.backward<span class="o">$</span>dW<span class="p">,</span> b1 <span class="o">=</span> Affine_1_layer.backward<span class="o">$</span>db<span class="p">,</span> W2 <span class="o">=</span> Affine_2_layer.backward<span class="o">$</span>dW<span class="p">,</span> b2 <span class="o">=</span> Affine_2_layer.backward<span class="o">$</span>db<span class="p">)</span>
    <span class="kr">return</span><span class="p">(</span>grads<span class="p">)</span>
<span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>이제 구현이 끝났으니 코드의 작동과 기능을 확인할 차례입니다.</p>

<h3 id="오차역전파법으로-구한-기울기-검증하기">오차역전파법으로 구한 기울기 검증하기</h3>

<p>오차역전파법으로 기울기를 구하는 방식을 구현했습니다. 이제 오차역전파법으로 구한 기울기가 정확한 값을 보이는지 기존의 수치미분 방식으로 구한 기울기 값과 비교해볼 차례입니다. 두 방식의 결과를 서로 비교하여 오차역전파법이 제대로 구현되었는지 확인하는 과정을 기울기 검증이라고 합니다. 기울기 검증 방법은 다음과 같습니다.</p>

<p>먼저 데이터를 불러옵니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="kn">library</span><span class="p">(</span>dslabs<span class="p">)</span>
<span class="kn">source</span><span class="p">(</span><span class="s">&#34;./DeepLearningFromForR/utils.R&#34;</span><span class="p">)</span>
<span class="kn">source</span><span class="p">(</span><span class="s">&#34;./DeepLearningFromForR/numerical_gradient.R&#34;</span><span class="p">)</span>

mnist_data <span class="o">&lt;-</span> get_data<span class="p">()</span>

making_one_hot_label <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>t_label<span class="p">,</span><span class="kp">nrow</span><span class="p">,</span><span class="kp">ncol</span><span class="p">){</span>
  data <span class="o">&lt;-</span> <span class="kt">matrix</span><span class="p">(</span><span class="kc">FALSE</span><span class="p">,</span>nrow <span class="o">=</span> <span class="kp">nrow</span><span class="p">,</span>ncol <span class="o">=</span> <span class="kp">ncol</span><span class="p">)</span>
  t_index <span class="o">&lt;-</span> t_label<span class="m">+1</span>
  <span class="kr">for</span><span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span><span class="kp">NROW</span><span class="p">(</span>data<span class="p">)){</span>
    data<span class="p">[</span>i<span class="p">,</span> t_index<span class="p">[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kc">TRUE</span>
  <span class="p">}</span>
  <span class="kr">return</span><span class="p">(</span>data<span class="p">)</span>
<span class="p">}</span>
t_train_onehotlabel <span class="o">&lt;-</span> making_one_hot_label<span class="p">(</span>mnist_data<span class="o">$</span>t_train<span class="p">,</span><span class="m">60000</span><span class="p">,</span><span class="m">10</span><span class="p">)</span>
t_test_onehotlabel <span class="o">&lt;-</span> making_one_hot_label<span class="p">(</span>mnist_data<span class="o">$</span>t_test<span class="p">,</span><span class="m">10000</span><span class="p">,</span><span class="m">10</span><span class="p">)</span>

x_batch <span class="o">&lt;-</span> mnist_data<span class="o">$</span>x_train<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">,]</span>

t_batch <span class="o">&lt;-</span> t_train_onehotlabel<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">,]</span></code></pre></td></tr></table>
</div>
</div>
<p>기울기를 비교할 데이터는 훈련데이터를 3개씩 뽑아 비교에 사용합니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r">W1 <span class="o">&lt;-</span> params<span class="o">$</span>W1
b1 <span class="o">&lt;-</span> params<span class="o">$</span>b1
W2 <span class="o">&lt;-</span> params<span class="o">$</span>W2
b2 <span class="o">&lt;-</span> params<span class="o">$</span>b2

model.backward.test <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">){</span>
  Affine_1_layer <span class="o">&lt;-</span> Affine.forward<span class="p">(</span>W1<span class="p">,</span> b1<span class="p">,</span> x<span class="p">)</span>
  Relu_1_layer <span class="o">&lt;-</span> Relu.forward<span class="p">(</span>Affine_1_layer<span class="o">$</span>out<span class="p">)</span>
  Affine_2_layer <span class="o">&lt;-</span> Affine.forward<span class="p">(</span>W2<span class="p">,</span> b2<span class="p">,</span> Relu_1_layer<span class="o">$</span>out<span class="p">)</span>
  <span class="kr">return</span><span class="p">(</span><span class="kt">list</span><span class="p">(</span>x  <span class="o">=</span>  Affine_2_layer<span class="o">$</span>out<span class="p">,</span> Affine_1.forward  <span class="o">=</span>  Affine_1_layer<span class="p">,</span> Affine_2.forward  <span class="o">=</span>  Affine_2_layer<span class="p">,</span> Relu_1.forward  <span class="o">=</span>  Relu_1_layer<span class="p">))</span>
<span class="p">}</span>

loss.test <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span><span class="kp">t</span><span class="p">){</span>
  temp  <span class="o">&lt;-</span> model.backward.test<span class="p">(</span>x<span class="p">)</span>
  y <span class="o">&lt;-</span> temp<span class="o">$</span>x
  last_layer.forward <span class="o">&lt;-</span> SoftmaxWithLoss.forward<span class="p">(</span>y<span class="p">,</span> <span class="kp">t</span><span class="p">)</span>
  <span class="kr">return</span><span class="p">(</span>last_layer.forward<span class="o">$</span>loss<span class="p">)</span>
<span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>순전파의 수치미분 값을 비교할 함수를 test란 이름으로 만들어줍니다. 이 함수들은  순전파에 사용하는 함수에 params 리스트 대신 객체 W1~b2로 변경한 것입니다. 이제 두 가지를 과정을 서로 비교해보겠습니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r">grad_numerical <span class="o">&lt;-</span> numerical_gradient<span class="p">(</span>loss.test<span class="p">,</span>x_batch<span class="p">,</span> t_batch<span class="p">)</span>
grad_backprop <span class="o">&lt;-</span> gradient<span class="p">(</span>x_batch<span class="p">,</span> t_batch<span class="p">)</span>

W1_diff <span class="o">&lt;-</span> <span class="kp">mean</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span>grad_backprop<span class="o">$</span>W1<span class="o">-</span>grad_numerical<span class="o">$</span>W1<span class="p">))</span>
b1_diff <span class="o">&lt;-</span> <span class="kp">mean</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span>grad_backprop<span class="o">$</span>b1<span class="o">-</span>grad_numerical<span class="o">$</span>b1<span class="p">))</span>
W2_diff <span class="o">&lt;-</span> <span class="kp">mean</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span>grad_backprop<span class="o">$</span>W2<span class="o">-</span>grad_numerical<span class="o">$</span>W2<span class="p">))</span>
b2_diff <span class="o">&lt;-</span> <span class="kp">mean</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span>grad_backprop<span class="o">$</span>b2<span class="o">-</span>grad_numerical<span class="o">$</span>b2<span class="p">))</span>

temp <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">(</span>W1 <span class="o">=</span> W1_diff<span class="p">,</span> b1 <span class="o">=</span> b1_diff<span class="p">,</span> W2 <span class="o">=</span> W2_diff<span class="p">,</span> b2 <span class="o">=</span> b2_diff<span class="p">)</span>
<span class="o">&gt;</span> temp

<span class="o">$</span>W1
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">4.020843e-10</span>

<span class="o">$</span>b1
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">2.226819e-09</span>

<span class="o">$</span>W2
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">5.757658e-09</span>

<span class="o">$</span>b2
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">1.400857e-07</span></code></pre></td></tr></table>
</div>
</div>
<p>먼저 수치미분 방식으로 기울기를 구해줍니다. 단 위에서 사용한 수치미분 방식에는 <code>sigmoid</code> 함수를 활성화 함수로 사용하지 않고 <code>Relu</code> 함수를 활성화 함수로 사용합니다. 또한, <code>softmax</code> 함수를 <code>cross_entropy_error</code> 함수와 함께 적용하는 순서의 차이 정도만 있습니다.</p>

<p>위의 코드를 통해 구한 기울기를 비교하면 0에 가까운 값이 도출됩니다. 곧 아주 미미한 차이가 나타남을 확인할 수 있습니다. 만약 기존의 시그모이드 함수를 사용한 순전파 과정으로 기울기를 구한다면 이러한 미미한 차이가 아닌 상당한 차이를 확인할 수 있기 때문에 주의해야합니다. 또한, 이 차이는 가중치의 랜덤값에 영향을 받기 때문에 비교 수치가 약간의 차이가 있을 수 있습니다.</p>

<p>수치 미분 방식과 오차역전파법을 통해 구한 기울기의 차이가 0이되는 일은 드물게 발생합니다. 왜냐하면 컴퓨터의 계산의 정밀도에 한계가 있기 때문입니다. 두 방식의 기울기의 차가 크면 오차역전파법을 잘못 구현했다고 볼 수 있습니다. 이제 오차역전파법과 수치미분 방식의 기울기가 서로 큰 차이가 없음을 확인했기 때문에 오차역전파법 기능 구현에 문제가 없음을 확인했다고 할 수 있습니다.</p>

<h3 id="오차역전파법-사용한-학습-구현하기">오차역전파법 사용한 학습 구현하기</h3>

<p>이제 오차역전파법을 통한 학습 과정을 알아보겠습니다. 학습과정은 순전파와 크게 다른 바가 없습니다. 차이점은 단지 역전파를 통해 기울기를 찾고 그 값으로 갱신한다는 점입니다. 먼저 필요한 객체들과, 훈련데이터 및 테스트셋을 준비합니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="kn">source</span><span class="p">(</span><span class="s">&#34;./DeepLearningFromForR/utils.R&#34;</span><span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>dslabs<span class="p">)</span>

mnist_data <span class="o">&lt;-</span> get_data<span class="p">()</span>

model.evaluate.backward <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span><span class="kp">t</span><span class="p">){</span>
    y <span class="o">&lt;-</span> <span class="kp">max.col</span><span class="p">(</span>model.backward<span class="p">(</span>x<span class="p">)</span><span class="o">$</span>x<span class="p">)</span>
    t <span class="o">&lt;-</span> <span class="kp">max.col</span><span class="p">(</span><span class="kp">t</span><span class="p">)</span>
    accuracy <span class="o">&lt;-</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span><span class="kp">ifelse</span><span class="p">(</span>y <span class="o">==</span> <span class="kp">t</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">0</span><span class="p">)))</span> <span class="o">/</span> <span class="kp">dim</span><span class="p">(</span>x<span class="p">)[</span><span class="m">1</span><span class="p">]</span>
    <span class="kr">return</span><span class="p">(</span>accuracy<span class="p">)</span>
<span class="p">}</span>

x_train_normalize <span class="o">&lt;-</span> mnist_data<span class="o">$</span>x_train
x_test_normalize <span class="o">&lt;-</span> mnist_data<span class="o">$</span>x_test

t_train_onehotlabel <span class="o">&lt;-</span> making_one_hot_label<span class="p">(</span>mnist_data<span class="o">$</span>t_train<span class="p">,</span><span class="m">60000</span><span class="p">,</span><span class="m">10</span><span class="p">)</span>
t_test_onehotlabel <span class="o">&lt;-</span> making_one_hot_label<span class="p">(</span>mnist_data<span class="o">$</span>t_test<span class="p">,</span><span class="m">10000</span><span class="p">,</span><span class="m">10</span><span class="p">)</span>


iters_num <span class="o">&lt;-</span> <span class="m">10000</span>
train_size <span class="o">&lt;-</span> <span class="kp">dim</span><span class="p">(</span>x_train_normalize<span class="p">)[</span><span class="m">1</span><span class="p">]</span>
batch_size <span class="o">&lt;-</span> <span class="m">100</span>
learning_rate <span class="o">&lt;-</span> <span class="m">0.1</span>

train_loss_list <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>lossvalue  <span class="o">=</span>  <span class="kp">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span>iters_num<span class="p">))</span>
train_acc_list <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>train_acc  <span class="o">=</span>  <span class="m">0</span><span class="p">)</span>
test_acc_list <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>test_acc  <span class="o">=</span>  <span class="m">0</span><span class="p">)</span>

iter_per_epoch <span class="o">&lt;-</span> <span class="kp">max</span><span class="p">(</span>train_size <span class="o">/</span> batch_size<span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<p>필요한 데이터가 준비되면 이제 모델의 학습을 시작할 수 있습니다. 모델 학습은 순전파 때와 다르게 시간이 적게걸립니다.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-r" data-lang="r"><span class="kr">for</span><span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span>iters_num<span class="p">){</span>
  batch_mask <span class="o">&lt;-</span> <span class="kp">sample</span><span class="p">(</span>train_size <span class="p">,</span>batch_size<span class="p">)</span>
  x_batch <span class="o">&lt;-</span> x_train_normalize<span class="p">[</span>batch_mask<span class="p">,]</span>
  t_batch <span class="o">&lt;-</span> t_train_onehotlabel<span class="p">[</span>batch_mask<span class="p">,]</span>

  grad <span class="o">&lt;-</span> gradient<span class="p">(</span>x_batch<span class="p">,</span> t_batch<span class="p">)</span>

  params<span class="o">$</span>W1 <span class="o">&lt;-</span> params<span class="o">$</span>W1 <span class="o">-</span> <span class="p">(</span>grad<span class="o">$</span>W1 <span class="o">*</span> learning_rate<span class="p">)</span>
  params<span class="o">$</span>W2 <span class="o">&lt;-</span> params<span class="o">$</span>W2 <span class="o">-</span> <span class="p">(</span>grad<span class="o">$</span>W2 <span class="o">*</span> learning_rate<span class="p">)</span>
  params<span class="o">$</span>b1 <span class="o">&lt;-</span> params<span class="o">$</span>b1 <span class="o">-</span> <span class="p">(</span>grad<span class="o">$</span>b1 <span class="o">*</span> learning_rate<span class="p">)</span>
  params<span class="o">$</span>b2 <span class="o">&lt;-</span> params<span class="o">$</span>b2 <span class="o">-</span> <span class="p">(</span>grad<span class="o">$</span>b2 <span class="o">*</span> learning_rate<span class="p">)</span>

  loss_value <span class="o">&lt;-</span> backward_loss<span class="p">(</span>x_batch<span class="p">,</span> t_batch<span class="p">)</span><span class="o">$</span>loss
  train_loss_list <span class="o">&lt;-</span> <span class="kp">rbind</span><span class="p">(</span>train_loss_list<span class="p">,</span>loss_value<span class="p">)</span>

  <span class="kr">if</span><span class="p">(</span>i <span class="o">%%</span> iter_per_epoch <span class="o">==</span> <span class="m">0</span><span class="p">){</span>
    train_acc <span class="o">&lt;-</span> model.evaluate.backward<span class="p">(</span>x_train_normalize<span class="p">,</span> t_train_onehotlabel<span class="p">)</span>
    test_acc <span class="o">&lt;-</span> model.evaluate.backward<span class="p">(</span>x_test_normalize<span class="p">,</span> t_test_onehotlabel<span class="p">)</span>
    train_acc_list <span class="o">&lt;-</span> <span class="kp">rbind</span><span class="p">(</span>train_acc_list<span class="p">,</span>train_acc<span class="p">)</span>
    test_acc_list <span class="o">&lt;-</span> <span class="kp">rbind</span><span class="p">(</span>test_acc_list<span class="p">,</span>test_acc<span class="p">)</span>
    <span class="kp">print</span><span class="p">(</span><span class="kt">c</span><span class="p">(</span>train_acc<span class="p">,</span> test_acc<span class="p">))</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.90545</span> <span class="m">0.91020</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9216667</span> <span class="m">0.9218000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9349667</span> <span class="m">0.9357000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9445167</span> <span class="m">0.9444000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9505333</span> <span class="m">0.9468000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9557333</span> <span class="m">0.9523000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9605</span> <span class="m">0.9570</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9639167</span> <span class="m">0.9603000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9664167</span> <span class="m">0.9630000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9696667</span> <span class="m">0.9635000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9710667</span> <span class="m">0.9655000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9725667</span> <span class="m">0.9671000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9736</span> <span class="m">0.9670</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9752</span> <span class="m">0.9689</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.9767833</span> <span class="m">0.9689000</span>
<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="m">0.97675</span> <span class="m">0.96910</span>

plot<span class="p">(</span>x <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">17</span><span class="p">,</span>y <span class="o">=</span> train_acc_list<span class="o">$</span>train_acc<span class="p">,</span><span class="s">&#34;l&#34;</span><span class="p">)</span>
lines<span class="p">(</span>x <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">17</span><span class="p">,</span>y <span class="o">=</span> test_acc_list<span class="o">$</span>test_acc<span class="p">,</span>col <span class="o">=</span> <span class="s">&#34;red&#34;</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<p><img src="https://user-images.githubusercontent.com/19144813/78006588-2b6c2980-7378-11ea-9496-e1dc7a1501c7.png" alt="정확도 곡선" /></p>

<p>이 값들은 가중치의 랜덤 값에 따라 달라지기에 실제로 실행시 약간의 차이가 있을 수 있습니다. 그러나. 퍼센트를 확인해보면 약 97%정도 정확도를 확인할 수 있습니다. 그래프를 통해서도 정확도가 0에서 97%까지 올라간 것을 확인할 수 있습니다. 이상으로 딥러닝 역전파 모델에 대한 구현을 마칩니다.</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Sunsick</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2020-03-30</span>
  </p>
  
  
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/r%EB%A1%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%95%98%EA%B8%B0/">R로 딥러닝하기</a>
          <a href="/tags/%EC%8B%A0%EA%B2%BD%EB%A7%9D/">신경망</a>
          <a href="/tags/%EC%97%AD%EC%A0%84%ED%8C%8C/">역전파</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/contents_list/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">R로 딥러닝하기 목차</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/neural_network_backward_3/">
            <span class="next-text nav-default">Chapter4 다양한 역전파 계층</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://www.google.com/" class="iconfont icon-google" title="google"></a>
      <a href="https://github.com/choosunsick" class="iconfont icon-github" title="github"></a>
  <a href="https://choosunsick.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Sunsick</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>








</body>
</html>
